# modelingClinicalTrialAttrition_NeuropsychiatricTrials
SQL script to pull neuropsychiatric trials / Python to clean and model data from the Aggregate Analysis of Clinicaltrials.gov (AACT)

## Abstract : 
Throughout the lifecycle of drug development, the clinical trial phases have a huge impact on the efficiency of the development and successful launch of that novel therapeutic in the marketplace. One of the major barriers of the successful completion of each clinical trial phase and eventual FDA approval is attrition of trial subjects. Clinical trial subject attrition levels directly affect the generalizability and validity of the results of a trial. If trials are lacking generalizability and validity the likelihood, they go to market and find success in the patients of interest decreases. To address this problem, we have pulled a unique dataset from the open-source database Aggregate Analysis of ClinicalTrials.gov (AACT) to create a sponsor-independent dataset representing neuropsychiatric clinical trials. The dataset pulled from the AACT included 16,220 trials of interest, of which 1,359 trials had complete data entered that supported the percent attrition target variable representing 377,378 trial subjects in the neuropsychiatric domain. To our knowledge this data-driven sponsor-independent approach to identify top features influencing subject attrition in neuropsychiatric trials is novel in the literature. Top features predicting clinical trial attrition included inclusion/exclusion criteria issue, study duration, lost to follow-up, and adverse event. Top regression models included Random Forest Regressor and Gradient Boosting Regressor with top performance at 79% accuracy (r2 = 0.79) score predicting percent attrition in the master neuropsychiatric clinical trial dataset.

## Introduction : 
The historical trend of bringing novel therapeutics to market has seen sharp, steady increases in time and money invested without a positive association of successful treatments reaching patients. The annual budget for Research and Development (R&D) teams in pharmaceutical companies have been reported by ClinicalTrials.gov to have increased 10-fold when compared to the budget in 1980, after adjusting for inflation (clinicalTrial.gov, 2022). The congressional Budget Office reported in 2021, that pharmaceutical companies spent approximately one quarter of their revenues on R&D in 2019 which was nearly double the size spent in this sector when compared to what was spent in 2000 (congressional budget office, 2021). This exceptional growth of industry has not yet translated into the same growth reflected in FDA approvals of said novel therapeutics. 
 
Researchers have shown that nine out of ten drug candidates fail during phases I, II, and III clinical trials (Sun et al., 2022; Takebe et al., 2018). Approximately 50% of that 90% failure rate in phases I-III trials can be attributed to lack of proving clinical efficacy (Harrison, 2016). A failure to prove efficacy in a clinical trial can be the result of many different factors; however, the existing literature has shown subject attrition is a major reason trials fail to prove efficacy and generalizability of final results. Patient recruitment and retention are major contributors to the amount of money spent, length of projected timeline, and successful outcome of a clinical trial. The successful enrollment in enough subjects of interest and retention of those subjects is critical to obtaining final evaluative data that supports statistical power to prove therapeutic effect. 
 
We are proposing with this project a sponsor agnostic, data-driven approach to address the problem of patient attrition in neuropsychiatric clinical trials using machine learning algorithms trained on publicly available data from the AACT database sponsored by the Clinical Trial Transformation Initiative (CTT) (Alexander, Corrigan-Curay, & McClellan, 2018; Zarin, Tse, Williams, Califf, & Ide, 2011). Machine learning is a key part of this project because of its excellent ability in the handling of large numbers of predictors - in this case, more predictors than observations - and allows for the combination of those predictors in a non-linear manner (Obermeyer, 2016). This machine learning approach is particularly important for attempting to predict something like patient attrition which has multiple factors involved in the process. 
 
If we can provide insight into key features leading to participant dropout in neuropsychiatric trials this research has the potential to provide that information to trial designers to aid in the creation of protocols that support subject retention. Better and more informed trial protocols that lead to higher rates of subject retention will undoubtedly result in more representative and higher-powered trials. Ultimately, leading to increased likelihood of trial success meaning better therapeutics for more patients in the marketplace. The models generated from this work could also be applied in a way that the researchers of a clinical trial enter in their assumptions for a future trial into the model feature fields (i.e., length of trial, number of arms, number of predicted serious adverse events) and get a prediction of the level of subject attrition they can expect with those entered features with a certain amount of accuracy. The application of the machine learning models from this project would allow for better planning of subject enrollment size in neuropsychiatric clinical trials keeping in mind accurate dropout rates to be expected and a smaller chance of encountering the negative effects associated with unaccounted for high attrition, attrition bias and underpowered results


## Order to view and run code :
 First pulled trials from AACT 
- AACT_Depression.Rmd
- AACT_Anxiety.Rmd
- AACT_Bipolar.Rmd
- AACT_Alzheimers.Rmd
- AACT_Parkinsons.Rmd

*this will pull generic features, dropout reasons, and adverse events for each individual disease type - 3 .csv files per disease type*

Next : Load AACT files into python, EDA, and export clean .csv files 
- V2_depression.ipynb
- V2_anxiety.ipynb
- V2_bipolar.ipynb
- V2_alzheimers.ipynb
- V2_parkinsons.ipynb

Next : NLP using K-Mean clustering on dropout reasons
- Master Dropout Reason Notebook.ipynb

Next : Adverse Events using MedDRA ontology mapping 
    - Load two files : 1. Distinct adverse reported events (distinct_reported_events.csv) 2. Standardised MedDRA Queries (smq_edited.csv)
- adverse_Events.ipynb 
- adverseEventTable.ipynb
    - Clean table so each individual trial has the number of affected patients per adverse event for that trial after adverse events have been recategorized from the free text.

Next : Compile all disease types into three distinct clean master files (generic features, adverse events, dropout reasons) 
- V3_Master.ipynb

Next : Execute modeling both with and without feature selection 
- test_feature.ipynb
- test_noFeature.ipynb

## Results 
We first generated a distribution of percent attrition by individual disease and master neuropsychiatric trials of our curated dataset to verify if our dataset was reflecting similar attrition levels as is in the literature. The percent dropout between the individual neuropsychiatric diseases were comparable with an average percent attrition between 20-30%. The master neuropsychiatric curated dataset had an average percent attrition of 26.5% and the individual diseases investigated including Depression (23.2%), Anxiety (19.2%), Bipolar disorder (35.9%), Alzheimer’s Disease (29.2%), Parkinson’s Disease (20.7%), and ALS (30.8%) were as reported. These approximate averages of percent attrition listed for our curated dataset is supported in the literature (Cooper et al., 2015; Ong et al., 2016; Hui et al., 2013). 
 
As we focused on neuropsychiatric trials, we next investigated the distribution of dropout reasons that a trial suffered. Those who were not re-classified using KMeans clustering into the top nine structured reasons for dropout were marked as “Unknown” which was one of the top reasons for dropout. The next most frequent reason for dropout was Adverse Events, Subject Withdrawals, and Protocol Violations.
 
### Regression Modeling Results
With the full structured, cleaned, and curated dataset we were able to fit multiple linear regression models to train and test our neuropsychiatric clinical trial dataset. The results presented below are those of the models after many iterations of readjusting grid search hyperparameters and optimized model selection. The most consistent and high accuracy results, as reported with r2 scores, came from the master curated dataset of all 6 individual neuropsychiatric diseases combined. This was anticipated because of the issues presented in the limitations section for limited record counts and size/shape of the data. The Random Forest Regressor model when tested on data without seeing the target variable had a 71% accuracy rate of predicting the correct percent attrition based on all the features from the original cleaned curated dataset. The Gradient Boosting Regressor model when tested on data without seeing the target variable had a 79% accuracy rate of predicting the correct percent attrition based on all the features from the original cleaned curated dataset. 
Many approaches of feature selection were performed. Gradient Boosting Machine selecting the top 10 most important features was the top performing approach when compared to the others, with a literature search knowledge-based approach as the second-best performing feature selection. After the gradient boosting machine approach selected the top 10 features to re-run the regression analysis the Random Forest Regressor model had a 75% accuracy of predicting percent attrition and Gradient boosting Regressor had a 72% accuracy. 
 
### Top Predictive Feature Results
Finally, we examined the topmost important features by individual disease type and the master neuropsychiatric trial dataset to see what specific features were important to predicting percent attrition. The topmost important features across three of the individual disease types as well as the master neuropsychiatric dataset was the dropout reason for inclusion/exclusions criteria issue. The features most common amongst the individual disease types as well as the master neuropsychiatric dataset were study duration, lost to follow-up, and adverse event. The adverse events that were identified in the top ten features for depression were headache, abdominal pain, and nausea. The adverse event that was identified in the top ten features for anxiety was fatigue. The adverse events that were identified in the top ten features for bipolar disorder were fatigue, overdose, and dystonia. The adverse events that were identified in the top ten features for Alzheimer’s Disease were pollakiuria, headache, and pyrexia. The adverse events that were identified in the top ten features for Parkinson’s Disease were urinary retention, hypertension, and dizziness. The adverse event that was identified in the top ten features for the master neuropsychiatric dataset was overdose.

### Limitations
The first limitation encountered in this study was the issue of incomplete data entry into the data source. As stated previously, there was an approximate 80% record loss since one of the two features used to calculate percent attrition, the target variable, was not entered into the clinicalTrials.gov website and transferred over to the AACT database. The number of unique clinical trial records were not adequate in supporting optimal modeling. ALS as an individual disease, was unable to be modeled with only 2 trials with complete data after cleaning. The necessity for many records when training a machine learning model meant that the other individual disease models, as well as the master neuropsychiatric model, were not likely to have optimal performances. In some instances, like for modeling Anxiety and Bipolar, the models had only 29 records to train and 8 to test. At most, modeling the master neuropsychiatric full trial dataset, the models had 640 records to train and 160 to test. 
 
The Second limitation of the dataset was the low number of records in comparison to all the clean features we wanted to test. With 808 clean features of interest, we would have ideally wanted a dataset with at least 8,080 records as the standard practice for fitting regression analysis is approximately 10 records per feature (Stoltfus, 2011). Most machine learning algorithms make assumptions that the record number is much higher than the feature count and therefore model performance deteriorates which may be another explanation for the performance of the regression models presented in the results section above. The limited amount of complete data available is a limitation of many machine learning projects, especially in the clinical space, where factors like patient data security and privacy along with researchers' apprehension to share data act as significant barriers to large amounts of clean usable data. 
 
The loss of granularity in the data was another limitation we encountered when classifying unstructured data. This issue was encountered when classifying the unstructured dropout reason text entered by researchers from the AACT database using unsupervised machine learning to identify clusters of reasons for dropout. Ultimately, using KMeans clustering, we identified nine categories of dropout reason and used those to restructure this portion of the data. The process of selecting nine distinct reasons for dropout introduces limitations like inability to identify several subjects that were dropped for multiple reasons, dropped for a reason that was slightly more unique than one of the top nine identified reasons, or loss of data meaning altogether due to original data entry mistake. The distribution of dropout reasons plotted by disease type, subjects affected, and percent attrition experienced per that data point are illustrated in Figure 5. There is an imbalance in the data with significantly more data entries for dropout reasons: Adverse Events, Subject Withdrawals, and Protocol Violations. There are fewer entries for dropout reasons: Consent withdrawal and Physician Decision. This imbalance should be kept in mind when interpreting the results and the most significant features. 
